# ğŸ” Noble Backend - LLM + MCP Powered Knowledge Extraction System

This project is a modular backend framework designed to handle intelligent content extraction and question answering using **Large Language Models (LLMs)** and a two-step **MCP (Model Context Protocol)** tool-based pipeline.

---

## ğŸ§  Project Overview

This project enables users to ask questions on any topic. Instead of relying solely on an LLMâ€™s static knowledge, the system dynamically scrapes relevant web content, processes it using specialized tools (MCP), and passes the results back to the LLM for a more accurate and grounded response.

### ğŸ“Œ Key Features

- Tool-driven LLM architecture using OpenAI or Groq LLMs
- Web scraping through a two-step process (via Firecrawl API or equivalent)
- Modular tool system for easy extensibility
- SSE-based communication between FastAPI backend and MCP service
- Dockerized setup with Docker Compose for seamless development

---

## ğŸ§­ Project Flow

1. **User Query:** A user sends a question to the FastAPI backend.
2. **LLM Decision:** The LLM evaluates the query and determines if a tool is needed.
3. **MCP Tool Execution:**
   - If needed, the tool is invoked via an SSE (Server-Sent Events) channel from the MCP server.
   - The tool processes the request (e.g., crawling the web or extracting in-depth internal links).
4. **LLM Response:**
   - The toolâ€™s output is returned to the LLM.
   - The LLM integrates this result into the final response and returns it to the user.

---

## ğŸ—‚ï¸ Folder Structure

```bash
noble_backend/
â”‚
â”œâ”€â”€ routes/
â”‚   â”œâ”€â”€ doitr/
â”‚   â”‚   â”œâ”€â”€ client.py
â”‚   â”‚   â””â”€â”€ config.py
â”‚   â”œâ”€â”€ MCP/
â”‚   â”‚   â”œâ”€â”€ crawler.py
â”‚   â”‚   â”œâ”€â”€ crawler2.py
â”‚   â”‚   â””â”€â”€ main.py         # FastMCP server
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ logger.py
â”‚   â”‚   â””â”€â”€ OpenAI.py
â”‚   â”œâ”€â”€ client.py           # SSE client for FastAPI to MCP connection
â”‚   â”œâ”€â”€ web_search.py       # Tool handler
â”‚
â”œâ”€â”€ main.py                 # FastAPI server entry
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ .env
â””â”€â”€ README.md

## ğŸš€ How to Run the Project

### 1. âœ… Prerequisites

Ensure you have the following installed:

- [Docker](https://www.docker.com/)
- [Docker Compose](https://docs.docker.com/compose/)
- A `.env` file with required secrets (OpenAI/Groq keys, Firecrawl key, etc.)

#### Example `.env` file:

```env
OPENAI_API_KEY=your_openai_key
FIRECRAWL_API_KEY=your_firecrawl_key
SERVER_HOST=localhost
SERVER_PORT=3001
SSE_PATH=/sse

## ğŸ³ Run with Docker Compose

To start the backend and MCP servers, run:

```bash
docker-compose up --build

## ğŸ”§ What This Will Do

- âœ… Start the **FastMCP** service on port `3001`
- âœ… Start the **FastAPI** backend service on port `8000`

---

## ğŸŒ Access the Services

- **FastAPI Backend**: [http://localhost:8000](http://localhost:8000)
- **MCP Server (SSE Endpoint)**: [http://localhost:3001/sse](http://localhost:3001/sse)


## âš™ï¸ Configuration Highlights

- ğŸŒ€ **FastAPI** uses a **lifespan hook** to initialize the `MCPClient`.
- ğŸ”— `MCPClient` connects to the **MCP server** at startup and fetches available tools.
- ğŸ§© Tools are **modular** and easy to **register** or **extend**.
- ğŸ”„ Communication between **FastAPI** and **MCP server** happens via **SSE (Server-Sent Events)** for real-time processing.

## ğŸ§ª Example Use Case

> **Question:** _"Explain Quantum Computing in simple terms with examples from recent articles"_

### ğŸ” Step-by-Step Flow

1. ğŸ§  **LLM** decides a web search tool is required.
2. ğŸ•¸ï¸ `crawler.py` performs top-level scraping of relevant links and summaries.
3. ğŸ” `crawler2.py` fetches deeper in-depth data from the internal links.
4. ğŸ“¤ The results are **fed back to the LLM**.
5. ğŸ§¾ LLM constructs a **well-grounded, real-time response** using fresh data.

---

## ğŸ› ï¸ Tech Stack

| Tech              | Usage                                              |
|-------------------|----------------------------------------------------|
| **FastAPI**       | Main backend server                                |
| **Python Asyncio**| Concurrency for SSE + tool execution               |
| **Docker**        | Containerization for consistent deployment         |
| **SSE**           | Real-time FastAPI â†” MCP server communication       |
| **LLMs**          | OpenAI / Groq via `langchain_groq`                 |
| **Firecrawl**     | Web crawling API for scraping and retrieval        |

